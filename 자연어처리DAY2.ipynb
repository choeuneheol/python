{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리DAY2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdtHEl11IUy4Kc2dY+nq5r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choeuneheol/python/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%ACDAY2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yGs16uzFvNl",
        "outputId": "71f0513e-9834-4c61-81c0-69e319f295d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words =['have','going','love','lives','dies','watched','has']\n",
        "print(words)\n",
        "print([lemmatizer.lemmatize(word) for word in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e44btGwF5jt",
        "outputId": "5ac276e6-efe0-496f-f65c-fd6d0612a1ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['have', 'going', 'love', 'lives', 'dies', 'watched', 'has']\n",
            "['have', 'going', 'love', 'life', 'dy', 'watched', 'ha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize('lives','n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HhgGAjj5Gsq6",
        "outputId": "2bf9f7f8-68f7-4fe6-cafc-667fea5b5894"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'life'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"This was not the map we found in Bill Bones's chest, but an accurate copy, complete in all thinsg--names and heights and soundings--with the single exception of the red crosses and the written notes\""
      ],
      "metadata": {
        "id": "EyVfRmNWHcoQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "YW2_H1YuILuw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_stemmer = PorterStemmer()\n",
        "l_stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "id": "GWTEPGjGIYPm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentence = word_tokenize(sentence)"
      ],
      "metadata": {
        "id": "quDd17OyIb7v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIn1GFahIpMv",
        "outputId": "db0990fb-8c51-409a-ebaa-9a9cdcca547a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Bill', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'thinsg', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([p_stemmer.stem(word) for word in tokenized_sentence])\n",
        "print([l_stemmer.stem(word) for word in tokenized_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OqpeZxCJLKA",
        "outputId": "ece15ddd-afc8-431d-ba54-1a2926e122cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'bill', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thinsg', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note']\n",
            "['thi', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'bil', 'bon', \"'s\", 'chest', ',', 'but', 'an', 'acc', 'cop', ',', 'complet', 'in', 'al', 'thinsg', '--', 'nam', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'exceiv', 'of', 'the', 'red', 'cross', 'and', 'the', 'writ', 'not']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "WsgWCOXF-7GZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='I was wndering if anyone out there could enlighten  me on this car'"
      ],
      "metadata": {
        "id": "W57AXVTYCZC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v_WNdqkyCmZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r= re.compile(\"ab.\")\n",
        "r.search('adefg')"
      ],
      "metadata": {
        "id": "mLJiUUsh-odb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"a[0]  \" "
      ],
      "metadata": {
        "id": "kA2d_xoD-7qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"Regular expression: A\n",
        "regular expression, regex or\n",
        "regexp[1](sometimes called a\n",
        "rational expression)[2][3] is, in\n",
        "theoretical computer science and\n",
        "formal language theory, a sequence\n",
        "of characters that define a search\n",
        "pattern\"\"\" #아무 영어 문자 사용\n",
        "\n",
        "p_text = re.sub('[^a-zA-Z]','',text)\n",
        "print(p_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBoS-dC9KwZD",
        "outputId": "d63afe35-e3d3-432e-fbd7-b2e64e87b878"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegularexpressionAregularexpressionregexorregexpsometimescalledarationalexpressionisintheoreticalcomputerscienceandformallanguagetheoryasequenceofcharactersthatdefineasearchpattern\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r= re.compile(\"a.c\") #a와c사이 한글자\n",
        "r.search(\"kkk\")\n",
        "r.search(\"abc\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMs4zm9E9PrD",
        "outputId": "6360d773-3165-4e3a-b2ee-d5e48d7df9f4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 3), match='abc'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r=re.compile(\"ab?c\")#?앞의 글자가 있을수도 없을수도 있음\n",
        "r.search(\"abbc\")\n",
        "r.search(\"abc\")\n",
        "r.search(\"ac\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpFlYnJTDSEN",
        "outputId": "86a24a99-c2e1-4f3c-a3ae-f9b0da210342"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(0, 2), match='ac'>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = re.compile(\"ab+c\") # *앞의 글자가 없을 수도 있을수도 있음, 단 뒤에 글자는 반드시 있어야함"
      ],
      "metadata": {
        "id": "Ooo8WIe-DlU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"100John    PROF\n",
        "101 James   STUD\n",
        "102 Mac   STUD\"\"\""
      ],
      "metadata": {
        "id": "_C0v_ZYrEAyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#공백 기준 분리\n",
        "re.split('\\s+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRzw8eUDEUL3",
        "outputId": "3fc80a4b-a0f5-4c50-b981-374055fcaca0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Regular',\n",
              " 'expression:',\n",
              " 'A',\n",
              " 'regular',\n",
              " 'expression,',\n",
              " 'regex',\n",
              " 'or',\n",
              " 'regexp[1](sometimes',\n",
              " 'called',\n",
              " 'a',\n",
              " 'rational',\n",
              " 'expression)[2][3]',\n",
              " 'is,',\n",
              " 'in',\n",
              " 'theoretical',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'and',\n",
              " 'formal',\n",
              " 'language',\n",
              " 'theory,',\n",
              " 'a',\n",
              " 'sequence',\n",
              " 'of',\n",
              " 'characters',\n",
              " 'that',\n",
              " 'define',\n",
              " 'a',\n",
              " 'search',\n",
              " 'pattern']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 숫자만 추출\n",
        "re.findall('\\d+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNiPuhzVEZYX",
        "outputId": "153a26be-c3fd-4c52-96f2-2e81b80cd8cf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1', '2', '3']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트가 대문자인 행 추출\n",
        "re.findall('[A-Z]',text)\n",
        "re.findall('[A-Z]{4}',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2wASiZTFOti",
        "outputId": "f9a102b7-9b2e-4b7a-ef02-6b8c66f4fa17"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#처음만 대문자 추출\n",
        "re.findall('[A-Z][a-z]+',text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTEVcx5DFkKn",
        "outputId": "6331c1f6-ae99-4b01-aba9-205969301b61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Regular']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regexp Tokenizer"
      ],
      "metadata": {
        "id": "LGMWDgalGI6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "text=\"\"\"Don't be fooled by the dark sounding name, Mr.Jone's Orphanage is as\n",
        "cheery as cheery goes for a pastry shop\"\"\""
      ],
      "metadata": {
        "id": "XOhzGatiGXTg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.api import TokenizerI\n",
        "# 특수문자 혹은 공백 기준 단어 및 숫자 분리\n",
        "tokenizer1 = RegexpTokenizer(\"[\\w]+\")\n",
        "print(tokenizer1.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJD2fTmXGjWi",
        "outputId": "a7ad8a72-d0fe-4cf3-ba2b-48bc306e69f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Regular', 'expression', 'A', 'regular', 'expression', 'regex', 'or', 'regexp', '1', 'sometimes', 'called', 'a', 'rational', 'expression', '2', '3', 'is', 'in', 'theoretical', 'computer', 'science', 'and', 'formal', 'language', 'theory', 'a', 'sequence', 'of', 'characters', 'that', 'define', 'a', 'search', 'pattern']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#공백 기준 단어 분리\n",
        "tokenizer2 = RegexpTokenizer(\"\\s+\",gaps=True)\n",
        "print(tokenizer2.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26Qmx0RGHLGa",
        "outputId": "44ea4932-45a4-46ed-fc30-78ef235000c7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Regular', 'expression:', 'A', 'regular', 'expression,', 'regex', 'or', 'regexp[1](sometimes', 'called', 'a', 'rational', 'expression)[2][3]', 'is,', 'in', 'theoretical', 'computer', 'science', 'and', 'formal', 'language', 'theory,', 'a', 'sequence', 'of', 'characters', 'that', 'define', 'a', 'search', 'pattern']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 텍스트 레이블 인톧깅ㄹ\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "raw_text = \"\"\"A barber is a person. a barber is good person. a barber is huge\n",
        " person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His \n",
        " barber kept his word. a barber kept his word. His barber kept his secret.\n",
        "  But keeping and keeping such a huge secret to himself was driving the barber crazy.\n",
        "   the barber went up a huge mountain.\"\"\"\n"
      ],
      "metadata": {
        "id": "q53X5jwhHhi1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 토큰화\n",
        "sentences = sent_tokenize(raw_text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmYDKlV2JGwG",
        "outputId": "256337e8-ef58-4581-caa4-310189091704"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A barber is a person.', 'a barber is good person.', 'a barber is huge\\n person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His \\n barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "preprocessed_sentences=[]\n",
        "stop_words = set(stopwords.words('english'))\n",
        "for sentence in sentences:\n",
        "  # 단어 토큰화\n",
        "  tokenized_sentence = word_tokenize(sentence)\n",
        "  result=[]\n",
        "  for word in tokenized_sentence:\n",
        "    word = word.lower()\n",
        "    if word not in stop_words:\n",
        "      if len(word)>2:\n",
        "        result.append(word)\n",
        "        if word not in vocab:\n",
        "          vocab[word] = 0\n",
        "        vocab[word] += 1\n",
        "  preprocessed_sentences.append(result)\n",
        "print(preprocessed_sentences)\n",
        "print(vocab)\n",
        "print(vocab['barber'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuP51mcRJdc0",
        "outputId": "bb0100c6-1e86-4c54-9f3d-f5403b5b2c37"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
            "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjyH8pOWLGeU",
        "outputId": "cfbdc486-08eb-4898-c88d-441573414589"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EEKfOo0XN4Lk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}